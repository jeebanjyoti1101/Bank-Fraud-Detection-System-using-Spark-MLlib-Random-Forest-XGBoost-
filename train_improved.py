"""
Advanced Fraud Detection Training with Class Imbalance Handling
Implements SMOTE, enhanced features, and optimized hyperparameters for 90%+ accuracy
"""

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import RobustScaler, LabelEncoder
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import classification_report, roc_auc_score, confusion_matrix, roc_curve
import xgboost as xgb
import lightgbm as lgb
from catboost import CatBoostClassifier
from imblearn.over_sampling import SMOTE
import joblib
import json
from datetime import datetime
import warnings
warnings.filterwarnings('ignore')

print("="*80)
print("üöÄ ADVANCED FRAUD DETECTION TRAINING (90%+ TARGET)")
print("="*80)

# ============================================================================
# 1. LOAD DATA
# ============================================================================
print("\nüìÇ Step 1: Loading dataset...")
df = pd.read_csv('data/Fraud.csv')
print(f"   Total transactions: {len(df):,}")
print(f"   Fraud cases: {df['isFraud'].sum():,} ({df['isFraud'].mean()*100:.2f}%)")
print(f"   Legitimate cases: {(~df['isFraud'].astype(bool)).sum():,}")

# ============================================================================
# 2. ENHANCED FEATURE ENGINEERING
# ============================================================================
print("\nüîß Step 2: Engineering 60+ enhanced features...")

# Encode categorical features first
from sklearn.preprocessing import LabelEncoder

le_city = LabelEncoder()
le_card = LabelEncoder()
le_exp = LabelEncoder()
le_gender = LabelEncoder()

df['city_encoded'] = le_city.fit_transform(df['City'])
df['card_encoded'] = le_card.fit_transform(df['Card Type'])
df['exp_encoded'] = le_exp.fit_transform(df['Exp Type'])
df['gender_encoded'] = le_gender.fit_transform(df['Gender'])

# Basic features
df['amount_log'] = np.log1p(df['amount'])
df['amount_sqrt'] = np.sqrt(df['amount'])

# Balance changes
df['balance_change'] = df['newbalanceOrig'] - df['oldbalanceOrg']
df['amount_to_balance_ratio'] = np.where(df['oldbalanceOrg'] > 0, 
                                          df['amount'] / df['oldbalanceOrg'], 0)

# Balance errors (CRITICAL FOR FRAUD DETECTION)
df['balance_error'] = np.abs(df['oldbalanceOrg'] - df['amount'] - df['newbalanceOrig'])
df['balance_error_ratio'] = np.where(df['oldbalanceOrg'] > 0,
                                      df['balance_error'] / df['oldbalanceOrg'], 0)
df['has_balance_error'] = (df['balance_error'] > 0.01).astype(int)
df['large_balance_error'] = (df['balance_error'] > 100).astype(int)

# Zero balance flags
df['zero_balance_before'] = (df['oldbalanceOrg'] == 0).astype(int)
df['zero_balance_after'] = (df['newbalanceOrig'] == 0).astype(int)

# Drain patterns (KEY FRAUD INDICATOR)
drain_ratio = np.where(df['oldbalanceOrg'] > 0, df['amount'] / df['oldbalanceOrg'], 0)
df['complete_drain'] = ((df['newbalanceOrig'] == 0) & (df['oldbalanceOrg'] > 0)).astype(int)
df['partial_drain'] = ((drain_ratio > 0.5) & (drain_ratio < 0.95)).astype(int)
df['high_drain_ratio'] = (drain_ratio >= 0.95).astype(int)
df['medium_drain_ratio'] = ((drain_ratio >= 0.5) & (drain_ratio < 0.95)).astype(int)
df['low_drain_ratio'] = ((drain_ratio > 0) & (drain_ratio < 0.5)).astype(int)
df['near_complete_drain'] = ((drain_ratio >= 0.80) & (drain_ratio < 0.95)).astype(int)

# Destination balance features (set to 0 since not available in this dataset)
df['exact_balance_match'] = 0
df['almost_exact_match'] = 0
df['balance_mismatch'] = 0
df['suspicious_zero_transaction'] = ((df['oldbalanceOrg'] == 0) & (df['amount'] > 0)).astype(int)

# Amount categorization
amount_quintile_thresholds = [0, 13000, 74000, 163000, 350000, float('inf')]
df['amount_quintile'] = pd.cut(df['amount'], bins=amount_quintile_thresholds, 
                                labels=[0,1,2,3,4], include_lowest=True).astype(int)

amount_decile_thresholds = [0, 5000, 13000, 30000, 74000, 120000, 163000, 250000, 350000, 500000, float('inf')]
df['amount_decile'] = pd.cut(df['amount'], bins=amount_decile_thresholds, 
                              labels=[0,1,2,3,4,5,6,7,8,9], include_lowest=True).astype(int)

# Large amount flags (KEY FRAUD INDICATOR)
df['is_large_amount'] = (df['amount'] > 200000).astype(int)
df['is_very_large'] = (df['amount'] > 500000).astype(int)
df['round_amount'] = (np.mod(df['amount'], 1000) == 0).astype(int)
df['round_large_amount'] = ((df['amount'] >= 100000) & (np.mod(df['amount'], 1000) == 0)).astype(int)
df['round_medium_amount'] = ((df['amount'] >= 10000) & (df['amount'] < 100000) & 
                              (np.mod(df['amount'], 1000) == 0)).astype(int)
df['odd_amount'] = (np.mod(df['amount'], 1000) != 0).astype(int)
df['small_amount'] = (df['amount'] < 1000).astype(int)

# Outlier features
amount_mean = df['amount'].mean()
amount_std = df['amount'].std()
df['amount_zscore'] = (df['amount'] - amount_mean) / (amount_std + 1e-10)
df['amount_outlier_99'] = (np.abs(df['amount_zscore']) > 2.58).astype(int)
df['amount_outlier_95'] = (np.abs(df['amount_zscore']) > 1.96).astype(int)
df['amount_outlier_90'] = (np.abs(df['amount_zscore']) > 1.64).astype(int)

balance_mean = df['oldbalanceOrg'].mean()
balance_std = df['oldbalanceOrg'].std()
df['balance_zscore'] = (df['oldbalanceOrg'] - balance_mean) / (balance_std + 1e-10)
df['balance_zscore_outlier'] = (np.abs(df['balance_zscore']) > 2.58).astype(int)
df['amount_zscore_outlier'] = (np.abs(df['amount_zscore']) > 2.58).astype(int)

# IQR-based outliers
amount_q1, amount_q3 = df['amount'].quantile(0.25), df['amount'].quantile(0.75)
amount_iqr = amount_q3 - amount_q1
df['amount_iqr_outlier'] = ((df['amount'] < amount_q1 - 1.5*amount_iqr) | 
                             (df['amount'] > amount_q3 + 1.5*amount_iqr)).astype(int)

balance_q1, balance_q3 = df['oldbalanceOrg'].quantile(0.25), df['oldbalanceOrg'].quantile(0.75)
balance_iqr = balance_q3 - balance_q1
df['balance_iqr_outlier'] = ((df['oldbalanceOrg'] < balance_q1 - 1.5*balance_iqr) | 
                              (df['oldbalanceOrg'] > balance_q3 + 1.5*balance_iqr)).astype(int)

df['extreme_outlier'] = ((df['amount_outlier_99'] == 1) | (df['balance_zscore_outlier'] == 1)).astype(int)

# Percentile features
balance_percentiles = [0, 1, 13000, 50000, 150000, 500000, 1000000, 5000000, float('inf')]
df['balance_percentile'] = pd.cut(df['oldbalanceOrg'], bins=balance_percentiles, 
                                   labels=[0, 10, 25, 50, 75, 90, 95, 99], include_lowest=True).astype(float)

amount_percentiles = [0, 5000, 13000, 30000, 74000, 150000, 350000, 500000, float('inf')]
df['amount_percentile'] = pd.cut(df['amount'], bins=amount_percentiles, 
                                  labels=[10, 25, 40, 50, 75, 85, 95, 99], include_lowest=True).astype(float)

df['percentile_diff'] = df['balance_percentile'] - df['amount_percentile']

# Transaction type features (HIGH PREDICTIVE POWER)
df['transfer_large'] = ((df['type'] == 'TRANSFER') & (df['amount'] > 200000)).astype(int)
df['transfer_medium'] = ((df['type'] == 'TRANSFER') & (df['amount'] >= 50000) & 
                         (df['amount'] <= 200000)).astype(int)
df['cashout_large'] = ((df['type'] == 'CASH_OUT') & (df['amount'] > 200000)).astype(int)
df['cashout_medium'] = ((df['type'] == 'CASH_OUT') & (df['amount'] >= 50000) & 
                        (df['amount'] <= 200000)).astype(int)
df['payment_large'] = ((df['type'] == 'PAYMENT') & (df['amount'] > 200000)).astype(int)
df['transfer_or_cashout'] = ((df['type'] == 'TRANSFER') | (df['type'] == 'CASH_OUT')).astype(int)
df['high_risk_type'] = df['transfer_or_cashout']
df['low_risk_type'] = ((df['type'] == 'PAYMENT') | (df['type'] == 'CASH_IN')).astype(int)

# Type risk scoring
type_risk_map = {'TRANSFER': 0.8, 'CASH_OUT': 0.8, 'DEBIT': 0.4, 'PAYMENT': 0.2, 'CASH_IN': 0.2}
df['type_risk_score'] = df['type'].map(type_risk_map)
df['risky_transaction'] = ((df['type_risk_score'] >= 0.8) & (df['amount'] > 100000)).astype(int)

# Advanced ratios
df['new_to_old_balance_ratio'] = np.where(df['oldbalanceOrg'] > 0,
                                          df['newbalanceOrig'] / df['oldbalanceOrg'], 0)
df['amount_balance_product'] = df['amount'] * df['oldbalanceOrg']
df['amount_balance_product_log'] = np.log1p(df['amount_balance_product'])
df['balance_change_pct'] = np.where(df['oldbalanceOrg'] > 0,
                                    (df['newbalanceOrig'] - df['oldbalanceOrg']) / df['oldbalanceOrg'], 0)
df['extreme_change'] = (np.abs(df['balance_change_pct']) > 0.95).astype(int)

# Type encoding
type_encoding_map = {'PAYMENT': 0, 'TRANSFER': 1, 'CASH_OUT': 2, 'DEBIT': 3, 'CASH_IN': 4}
df['type_encoded'] = df['type'].map(type_encoding_map)

# Dataset source (all from same source in this case)
df['dataset_source_encoded'] = 0

# **NEW: CRITICAL FRAUD PATTERN FEATURES**
df['large_transfer_drain'] = ((df['type'] == 'TRANSFER') & 
                               (df['amount'] > 200000) & 
                               (df['complete_drain'] == 1)).astype(int)
df['cashout_drain'] = ((df['type'] == 'CASH_OUT') & 
                        (df['amount'] > 200000) & 
                        (df['complete_drain'] == 1)).astype(int)
df['complete_drain_large'] = ((df['complete_drain'] == 1) & (df['amount'] > 200000)).astype(int)
df['error_and_large'] = ((df['has_balance_error'] == 1) & (df['amount'] > 200000)).astype(int)

# City risk scoring (based on fraud patterns)
city_fraud_rate = df.groupby('City')['isFraud'].mean()
df['city_risk_score'] = df['City'].map(city_fraud_rate)

# Card type risk scoring
card_fraud_rate = df.groupby('Card Type')['isFraud'].mean()
df['card_risk_score'] = df['Card Type'].map(card_fraud_rate)

# Expense type risk scoring
exp_fraud_rate = df.groupby('Exp Type')['isFraud'].mean()
df['exp_risk_score'] = df['Exp Type'].map(exp_fraud_rate)

print(f"   ‚úÖ Created {len([col for col in df.columns if col not in ['Date', 'nameOrig', 'isFraud', 'City', 'type', 'Card Type', 'Exp Type', 'Gender']])} features")

# ============================================================================
# 3. SELECT FEATURES AND TARGET
# ============================================================================
print("\nüìã Step 3: Preparing feature set...")

# Exclude non-feature columns
exclude_cols = ['Date', 'nameOrig', 'isFraud', 'City', 'type', 'Card Type', 'Exp Type', 'Gender']
feature_cols = [col for col in df.columns if col not in exclude_cols]

X = df[feature_cols]
y = df['isFraud']

print(f"   Features: {len(feature_cols)}")
print(f"   Samples: {len(X):,}")

# ============================================================================
# 4. TRAIN-TEST SPLIT
# ============================================================================
print("\n‚úÇÔ∏è  Step 4: Splitting data (80/20)...")
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, 
                                                      random_state=42, stratify=y)
print(f"   Train: {len(X_train):,} | Test: {len(X_test):,}")
print(f"   Train fraud: {y_train.sum():,} ({y_train.mean()*100:.2f}%)")

# ============================================================================
# 5. HANDLE CLASS IMBALANCE WITH SMOTE
# ============================================================================
print("\n‚öñÔ∏è  Step 5: Handling class imbalance with SMOTE...")
print(f"   Before SMOTE:")
print(f"      Legitimate: {(y_train == 0).sum():,}")
print(f"      Fraud: {(y_train == 1).sum():,}")

sm = SMOTE(random_state=42, k_neighbors=5)
X_train_balanced, y_train_balanced = sm.fit_resample(X_train, y_train)

print(f"   After SMOTE:")
print(f"      Legitimate: {(y_train_balanced == 0).sum():,}")
print(f"      Fraud: {(y_train_balanced == 1).sum():,}")
print(f"   ‚úÖ Dataset balanced!")

# ============================================================================
# 6. SCALE FEATURES
# ============================================================================
print("\nüìä Step 6: Scaling features...")
scaler = RobustScaler()
X_train_scaled = scaler.fit_transform(X_train_balanced)
X_test_scaled = scaler.transform(X_test)
print("   ‚úÖ Features scaled with RobustScaler")

# ============================================================================
# 7. TRAIN MODELS WITH OPTIMIZED HYPERPARAMETERS
# ============================================================================
print("\n" + "="*80)
print("üéØ TRAINING 4 OPTIMIZED MODELS")
print("="*80)

# Calculate scale_pos_weight for imbalanced original data
scale_pos_weight = (y_train == 0).sum() / (y_train == 1).sum()
print(f"\nüìä Scale pos weight: {scale_pos_weight:.2f}")

# 7.1 Random Forest with class_weight='balanced'
print("\nüå≤ Model 1: Random Forest (Balanced)")
rf_model = RandomForestClassifier(
    n_estimators=300,
    max_depth=35,
    min_samples_split=5,
    min_samples_leaf=2,
    max_features='sqrt',
    class_weight='balanced',  # CRITICAL FOR IMBALANCE
    n_jobs=-1,
    random_state=42,
    verbose=1
)
rf_model.fit(X_train_scaled, y_train_balanced)
rf_pred = rf_model.predict(X_test_scaled)
rf_proba = rf_model.predict_proba(X_test_scaled)[:, 1]
rf_auc = roc_auc_score(y_test, rf_proba)
print(f"\n   ‚úÖ RF AUC: {rf_auc:.4f}")
print(classification_report(y_test, rf_pred, target_names=['Legitimate', 'Fraud']))

# 7.2 XGBoost with scale_pos_weight
print("\nüöÄ Model 2: XGBoost (Scale Pos Weight)")
xgb_model = xgb.XGBClassifier(
    n_estimators=300,
    max_depth=20,
    learning_rate=0.05,
    subsample=0.8,
    colsample_bytree=0.8,
    gamma=1,
    reg_alpha=0.5,
    reg_lambda=1,
    scale_pos_weight=scale_pos_weight,  # CRITICAL FOR IMBALANCE
    tree_method='hist',
    n_jobs=-1,
    random_state=42,
    eval_metric='auc'
)
xgb_model.fit(X_train_scaled, y_train_balanced)
xgb_pred = xgb_model.predict(X_test_scaled)
xgb_proba = xgb_model.predict_proba(X_test_scaled)[:, 1]
xgb_auc = roc_auc_score(y_test, xgb_proba)
print(f"\n   ‚úÖ XGB AUC: {xgb_auc:.4f}")
print(classification_report(y_test, xgb_pred, target_names=['Legitimate', 'Fraud']))

# 7.3 LightGBM with is_unbalance=True
print("\n‚ö° Model 3: LightGBM (Is Unbalance)")
lgb_model = lgb.LGBMClassifier(
    n_estimators=300,
    max_depth=20,
    learning_rate=0.05,
    num_leaves=31,
    subsample=0.8,
    colsample_bytree=0.8,
    reg_alpha=0.5,
    reg_lambda=1,
    is_unbalance=True,  # CRITICAL FOR IMBALANCE
    n_jobs=-1,
    random_state=42,
    verbose=-1
)
lgb_model.fit(X_train_scaled, y_train_balanced)
lgb_pred = lgb_model.predict(X_test_scaled)
lgb_proba = lgb_model.predict_proba(X_test_scaled)[:, 1]
lgb_auc = roc_auc_score(y_test, lgb_proba)
print(f"\n   ‚úÖ LGB AUC: {lgb_auc:.4f}")
print(classification_report(y_test, lgb_pred, target_names=['Legitimate', 'Fraud']))

# 7.4 CatBoost with auto_class_weights='Balanced'
print("\nüê± Model 4: CatBoost (Auto Class Weights)")
cat_model = CatBoostClassifier(
    iterations=300,
    depth=10,
    learning_rate=0.05,
    l2_leaf_reg=3,
    loss_function='Logloss',
    eval_metric='AUC',
    auto_class_weights='Balanced',  # CRITICAL FOR IMBALANCE
    random_seed=42,
    verbose=0,
    thread_count=-1
)
cat_model.fit(X_train_scaled, y_train_balanced)
cat_pred = cat_model.predict(X_test_scaled)
cat_proba = cat_model.predict_proba(X_test_scaled)[:, 1]
cat_auc = roc_auc_score(y_test, cat_proba)
print(f"\n   ‚úÖ CAT AUC: {cat_auc:.4f}")
print(classification_report(y_test, cat_pred, target_names=['Legitimate', 'Fraud']))

# ============================================================================
# 8. WEIGHTED ENSEMBLE WITH OPTIMIZED WEIGHTS
# ============================================================================
print("\n" + "="*80)
print("üéØ CREATING WEIGHTED ENSEMBLE")
print("="*80)

# Weight by AUC performance (better models get more weight)
aucs = [rf_auc, xgb_auc, lgb_auc, cat_auc]
total_auc = sum(aucs)
weights = [auc / total_auc for auc in aucs]

print(f"\nüìä AUC-Based Weights:")
print(f"   Random Forest: {weights[0]:.3f} (AUC: {rf_auc:.4f})")
print(f"   XGBoost:       {weights[1]:.3f} (AUC: {xgb_auc:.4f})")
print(f"   LightGBM:      {weights[2]:.3f} (AUC: {lgb_auc:.4f})")
print(f"   CatBoost:      {weights[3]:.3f} (AUC: {cat_auc:.4f})")

# Create weighted ensemble
ensemble_proba = (
    weights[0] * rf_proba +
    weights[1] * xgb_proba +
    weights[2] * lgb_proba +
    weights[3] * cat_proba
)

# Find optimal threshold using ROC curve
fpr, tpr, thresholds = roc_curve(y_test, ensemble_proba)
optimal_idx = np.argmax(tpr - fpr)
optimal_threshold = thresholds[optimal_idx]

print(f"\nüéØ Optimal Threshold: {optimal_threshold:.4f}")

# Apply optimized threshold
ensemble_pred = (ensemble_proba >= optimal_threshold).astype(int)
ensemble_auc = roc_auc_score(y_test, ensemble_proba)

print(f"\nüìä ENSEMBLE RESULTS:")
print(f"   AUC: {ensemble_auc:.4f}")
print(classification_report(y_test, ensemble_pred, target_names=['Legitimate', 'Fraud']))

# Confusion matrix
cm = confusion_matrix(y_test, ensemble_pred)
print(f"\nüìä Confusion Matrix:")
print(f"   True Negatives:  {cm[0,0]:,}")
print(f"   False Positives: {cm[0,1]:,}")
print(f"   False Negatives: {cm[1,0]:,}")
print(f"   True Positives:  {cm[1,1]:,}")

# ============================================================================
# 9. SAVE MODELS
# ============================================================================
print("\nüíæ Step 9: Saving models...")

joblib.dump(rf_model, 'models/rf_model.pkl')
joblib.dump(xgb_model, 'models/xgboost_model.pkl')
joblib.dump(lgb_model, 'models/lightgbm_model.pkl')
joblib.dump(cat_model, 'models/catboost_model.pkl')
joblib.dump(scaler, 'models/scaler.pkl')

# Save encoders
encoders = {
    'city': le_city,
    'card': le_card,
    'exp': le_exp,
    'gender': le_gender
}
joblib.dump(encoders, 'models/encoders.pkl')

# Save metadata
metadata = {
    'training_date': datetime.now().isoformat(),
    'training_samples': len(X_train_balanced),
    'test_samples': len(X_test),
    'features': len(feature_cols),
    'feature_names': feature_cols,
    'optimal_threshold': float(optimal_threshold),
    'ensemble_weights': {
        'random_forest': float(weights[0]),
        'xgboost': float(weights[1]),
        'lightgbm': float(weights[2]),
        'catboost': float(weights[3])
    },
    'models': {
        'random_forest': {
            'auc': float(rf_auc)
        },
        'xgboost': {
            'auc': float(xgb_auc)
        },
        'lightgbm': {
            'auc': float(lgb_auc)
        },
        'catboost': {
            'auc': float(cat_auc)
        },
        'ensemble': {
            'auc': float(ensemble_auc)
        }
    }
}

with open('models/advanced_metadata.json', 'w') as f:
    json.dump(metadata, f, indent=2)

print("   ‚úÖ Models saved!")

print("\n" + "="*80)
print("üéâ TRAINING COMPLETE!")
print("="*80)
print(f"\nüìä Final Results:")
print(f"   Ensemble AUC: {ensemble_auc:.4f}")
print(f"   Optimal Threshold: {optimal_threshold:.4f}")
print(f"   Target: AUC > 0.90, Recall > 0.85")
print(f"   Status: {'‚úÖ ACHIEVED!' if ensemble_auc > 0.90 else '‚ö†Ô∏è NEEDS IMPROVEMENT'}")
